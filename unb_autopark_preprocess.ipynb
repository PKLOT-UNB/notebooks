{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto: UNB - AutoPark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O dataset\n",
    "\n",
    "\n",
    "\n",
    "A preparação dos dados sera feita para que tenhamos todas as informaçoes relevantes de cada uma das imagens em um unico registro. Os metadados de cada uma das imagens estarão reunidados em um unico Dataframe e sera pesistido através de um arquivo CSV.\n",
    "\n",
    "As imagens do dataset serão processadas para que seu lado de maior dimensão seja receba um redimensionamento para a maior dimensão entre todas do conjunto de dados. Para a dimensão da imagem que não esta na maior possivel do dataset será aplicado um padding com a cor preta. Esta nova imagem será salva em um novo diretório ja pré processada.\n",
    "\n",
    "\n",
    "Assim cada um dos registros sera feito desta forma:\n",
    "\n",
    "\n",
    "relative_path   | parkinglot_name | date | weather | status \n",
    "--------- | ------ | ------ | ------ | ------ \n",
    "str | str | str | str | bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import imutils\n",
    "DATA_PATH = \"/home/delll/fga/veiculos_autonomos/datasets/PKLot/PKLotSegmented\"\n",
    "NEW_DATA_PATH = \"/home/delll/fga/veiculos_autonomos/datasets/PKLot/ready2go/\"\n",
    "\n",
    "# Maiores dimensões possiveis na base de dados\n",
    "bigger_w = 176\n",
    "bigger_h = 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(img, bigger_w, bigger_h):\n",
    "    \n",
    "    padding_sides = (bigger_w - img.shape[0])/ 2\n",
    "    if(padding_sides.is_integer() ==  False):\n",
    "        padding_sides = int(padding_sides - 0.5)\n",
    "    else:\n",
    "        padding_sides = int(padding_sides)\n",
    "\n",
    "    padding_top = (bigger_h - img.shape[1])/2    \n",
    "    if(padding_top.is_integer() ==  False):\n",
    "        padding_top = int(padding_top - 0.5)\n",
    "    else:\n",
    "        padding_top = int(padding_top)\n",
    "\n",
    "    result = np.ones([bigger_w, bigger_h, 3])\n",
    "    try:\n",
    "        result[padding_sides: padding_sides+img.shape[0], padding_top: padding_top + img.shape[1]] = np.array(img)\n",
    "    except:\n",
    "        return []\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_dataset_dir(parkinglot_name, weather, bigger_w, bigger_h, new_images_path):\n",
    "    images_path = DATA_PATH + \"/\"+ parkinglot_name + \"/\" + weather + \"/\"\n",
    "\n",
    "    for date in os.listdir(images_path):\n",
    "        dir_info_list= []\n",
    "\n",
    "        for state in os.listdir(images_path + date +\"/\"):\n",
    "\n",
    "            for file in os.listdir(images_path + date +\"/\" + state):\n",
    "                # path to image file\n",
    "                \n",
    "                file_path = images_path + date +\"/\" + state + \"/\" + file\n",
    "                new_file_path = os.path.join(new_images_path , file)\n",
    "\n",
    "                img = cv2.imread(file_path)\n",
    "                # make padding and save image\n",
    "                pw, ph = img.shape[0]/176, img.shape[1]/93\n",
    "                n_max_size = (110, 58)\n",
    "                if (pw > ph):\n",
    "                    img = imutils.resize(img, height=110)\n",
    "                elif (ph >= pw):\n",
    "                    img = imutils.resize(img, width=58)\n",
    "                new_img = pad_image(img, n_max_size[0], n_max_size[1])\n",
    "                if new_img == []:\n",
    "                    continue  \n",
    "                cv2.imwrite(new_file_path, new_img)\n",
    "                dir_info_list.append([ new_file_path,  parkinglot_name, date, weather,  state ])\n",
    "                \n",
    "                # should be changed to a log function\n",
    "                print(\"Leitura da imagem com sucesso: \", new_file_path.split(\"/\")[-1],  parkinglot_name, date, weather, state)\n",
    "    return dir_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puc_cloudy = read_dataset_dir(\"PUC\", \"Cloudy\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "puc_sunny = read_dataset_dir(\"PUC\", \"Sunny\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "puc_rainy = read_dataset_dir(\"PUC\", \"Rainy\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "\n",
    "UFPR04_cloudy = read_dataset_dir(\"UFPR04\", \"Cloudy\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "UFPR04_sunny  = read_dataset_dir(\"UFPR04\", \"Sunny\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "UFPR04_rainy  = read_dataset_dir(\"UFPR04\", \"Rainy\", bigger_w, bigger_h, NEW_DATA_PATH)            \n",
    "\n",
    "UFPR05_cloudy = read_dataset_dir(\"UFPR05\", \"Cloudy\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "UFPR05_sunny = read_dataset_dir(\"UFPR05\", \"Sunny\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "UFPR05_rainy = read_dataset_dir(\"UFPR05\", \"Rainy\", bigger_w, bigger_h, NEW_DATA_PATH)\n",
    "\n",
    "\n",
    "info_list = puc_sunny + puc_cloudy + puc_rainy + UFPR04_cloudy + UFPR04_sunny + UFPR04_rainy + UFPR05_cloudy + UFPR05_sunny + UFPR05_rainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=info_list, columns=[ \"relative_path\", \"parkinglot_name\", \"date\", \"weather\", \"status\" ])\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(df['relative_path'][6])\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendimento dos dados\n",
    "\n",
    "Após o pré processamento prodemos realizar algumas operações e ferramentas de vizualização para o melhor entendimendo dos dados. Iremos utilizar dos metadados obtidos na etapa anterior deste notebook. Não será necessário o carregamento desta imagens para esta etapa, mas para melhor eficiencia deste notebook iremos carregar as imagens agora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, element in enumerate(df.iterrows()):\n",
    "\n",
    "    img = cv2.imread(element[1]['relative_path'])\n",
    "\n",
    "    #normalizando as imagens, colocando em um intervalo de 0 a 1\n",
    "    X.append(img)\n",
    "    label = 0 if element[1]['status'] == 'Occupied' else 1  \n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Dataframe\n",
    "Agora, com o data frame carregado podemos ver um exemplo de seus primeiros elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[0 : int(len(X)*0.75)], X[int(len(X)*0.75): len(X)]\n",
    "y_train, y_test = y[0 : int(len(y)*0.75)], y[int(len(y)*0.75): len(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(110, 58, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).ravel()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acuracia obtida:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Test Data (area = {:.3f})'.format(auc))\n",
    "# plt.plot(fpr_train, tpr_train, label='Train Data (area = {:.3f})'.format(auc_train))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste pratico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"/home/delll/fga/veiculos_autonomos/datasets/imagens_teste_manual/3.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(test_image_path)\n",
    "img = cv2.imread(df['relative_path'][6])\n",
    "\n",
    "pw, ph = img.shape[0]/176, img.shape[1]/93\n",
    "n_max_size = (110, 58)\n",
    "if (pw > ph):\n",
    "    img = imutils.resize(img, height=110)\n",
    "else:\n",
    "    img = imutils.resize(img, width=58)\n",
    "    \n",
    "padding_sides = (110 - img.shape[0])/ 2\n",
    "if(padding_sides.is_integer() ==  False):\n",
    "    padding_sides = int(padding_sides - 0.5)\n",
    "else:\n",
    "    padding_sides = int(padding_sides)\n",
    "\n",
    "padding_top = (58 - img.shape[1])/2    \n",
    "if(padding_top.is_integer() ==  False):\n",
    "    padding_top = int(padding_top - 0.5)\n",
    "else:\n",
    "    padding_top = int(padding_top)\n",
    "\n",
    "result = np.ones([110, 58, 3])\n",
    "result[padding_sides: padding_sides+img.shape[0], padding_top: padding_top + img.shape[1]] = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.ones([110, 58, 3])\n",
    "result[padding_sides: padding_sides+img.shape[0], padding_top: padding_top + img.shape[1]] = img\n",
    "result = result.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result.reshape([1, 110, 58, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
